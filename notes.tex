\documentclass[10pt,landscape]{article}
\usepackage[landscape]{geometry}
\usepackage{multicol}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xfrac}
\usepackage{graphicx}

% Make the margins tight
\geometry{top=1cm, right=1cm, bottom=1cm, left=1cm}

% remove headers and footers
\pagestyle{empty}

% Make paragraph splits smaller
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5ex}

% Make the section headings smaller, inspired by the class Latex2e
% quick reference sheet.
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0pt}%
                        {-1.3ex}{0.7ex}{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0pt}%
                           {-1.3ex}{0.7ex}{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0pt}%
                              {-1.3ex}{0.7ex}{\normalfont\small\bfseries}}
\makeatother

% Tighten-Up lists
\let\oldenumerate\enumerate
\renewcommand{\enumerate}{%
    \oldenumerate%
    \setlength{\itemsep}{-3pt}%
}

\let\olditemize\itemize
\renewcommand{\itemize}{%
    \olditemize%
    \setlength{\itemsep}{-3pt}%
}

\let\olddescription\description
\renewcommand{\description}{%
    \olddescription%
    \setlength{\itemsep}{-3pt}%
}

% Remove section numbers
\setcounter{secnumdepth}{0}

% Utility commands
\newcommand{\spc}{,\;}
\newcommand{\spto}{\;\to\;}
\newcommand{\spbar}{\;|\;}
\newcommand{\spbackslash}{\;\backslash\;}
\newcommand{\impl}{\Rightarrow}

\newcommand{\langof}{\mathcal{L}}
\newcommand{\property}{\mathcal{P}}

% temp length variable
\newlength{\templength}

\begin{document}
\begin{multicols*}{3}

\begin{center}
    \Large CS3100 Final Note Sheet
\end{center}
\vspace{-0.5cm}

\section{Notation}
\begin{tabular}{rp{0.79\linewidth}}
$\overline{L}$, $L^c$ & Negation of language $L$. \\
$L^R$ & The reversal of language $L$. \\
$L^*$ & The Kleene-Star of $L$. \\
$AB$ & The concatenation of language $A$ and $B$. \\
$h(L)$ & A homomorphism (a function that maps every input to 
         a unique output) of $L$. \\
$A \spbackslash B$ & Set difference of $A$ and $B$. 
                     $A - B$ is the same thing. Equivalent to 
                     $A \cap \overline{B}$.\\
$2^A$ & The power-set of set $A$. \\
$\langof(A)$ & The language of machine (CFG, PDA, DFA, TM, \ldots) $A$. \\
\end{tabular}
$f : x_1, x_2, \ldots, x_n \to y_1, y_2, \ldots, y_n$ denotes
a function $f$ that when given $x_1, \ldots, x_n$ as inputs
yields $y_1, \ldots, y_n$ as outputs.

\section{Chomsky Hierarchy}
\begin{center}
    \includegraphics[width=0.6\linewidth]{images/chomsky-hierarchy}
\end{center}

\section{Regular Languages}
\input{regular}

\section{Context Free Languages}
\input{context_free}

% Cardinality
% (mostly done) Schroder-Bernstein theorem
\section{Cardinality}
\subsection{Diagonalization}
Diagonalization is a general method for showing that the cardinality of
two infinite sets is not the same. So say that we have two infinite sets $A$ and $B$,
in this particular case, lets consider $B = 2^A$. Now, by definition $B$ can
be expressed as a set of infinite sets. We can begin writing out these sets,
$s_1, s_2, \ldots, s_n$. Diagonalization tells us that no matter how many
of these sets we make, there will always exists another set $s$ that has not yet
been expressed. This set $s$ is constructed by going through the list of created
sets $s_1, s_2, \ldots s_n$ and putting an element in $s$ that does not exists in
each set, this way, it cannot ever be equal to another set because it
contains at least one element that is guaranteed to not be in that set.

\subsection{Schr\"{o}der-Bernstein Theorem}
The Schr\"{o}der-Bernstein theorem states that, for any two sets $A$ and $B$
if there exists an \textit{injective} function from $A \to B$, and
there exits an injective function from $B \to A$, then $|A| = |B|$.
Note that the injective function doesn't require every item of $A$ to map
to every item of $B$, only that every item of $A$ maps to \textit{an} item
of $B$ (and vice versa).
\subsection{Pairing Functions}

% Turing Machines
\section{Turing Machines}
Formaly a Turing machine is represented as tuple in the form 
$(Q, \Sigma, \Gamma, \delta, q_0, B, F)$. $Q, \Sigma, q_0$, and $F$ are the same
for PDAs and DFAs. $\Gamma$ is the tape alphabet, and since the input string
is presented on the tape, $\Sigma \subset \Gamma$ is always the case. $B$ is
the blank character. The $\delta$ is defined like so:
\[
    \delta : Q \times \Gamma \spto Q \times \Gamma \times \{L, R\}
\]
Which means the delta functions maps pairs of states an inputs, to a new state,
a value to replace the current value on the tape, and \textit{movement}. $L$ moves
the head left on the taps, and $R$ moves the head right on the tape. Since `staying-put'
can be trivially simulated $S$ is sometimes included as a movement as well. TMs
can also be non-deterministic, in that case the $\delta$ function changes to:
\[
    \delta : Q \times \Gamma \spto 2^{Q \times \Gamma \times \{L, R\}}
\]
Which allows it to be in multiple states at the same time, since it can transition
to multiple states with a single input.

\subsection{Terminology and Notation}
\begin{tabular}{rp{0.71\linewidth}}
$\langle M \rangle$ & String representation of Turing machine $M$. \\
halting & When a machine stops execution. \\
acceptance & When a machine halts in a final state. \\
rejection & When a machine halts and is not in a final state. \\
decider & A decider is a Turing machine that defines a language
          of Turing machines that conform to a yes or no question. \\
\end{tabular}

% Decidability
\section{Decidability}
\subsection{Recognizable and Enumerable}
\begin{description}
    \item[{\small Turing Recognizable (TR):}] A language $L$ where there exists some
    Turing machine $M$ that can recognize it.
    \item[{\small Recursively Enumerable (RE):}] A language $L$ where all strings in
    the language can be enumerated by some machine $M$.
\end{description}

Every language that is Turing Recognizable is also Recursively Enumerable since
an enumerator can be built from a recognizer, and a recognizer can be built
using an enumerator.

\subsection{Halting Problem}
The halting problem states building a Turing machine $P$ that can detect
whether any other Turing machine will halt is impossible. The proof is as
follows:

Assume that we have a Turing machine $P$, that when given a Turing machine
$M$ and string $w$ as input ($\langle M, w \rangle$), $P$ will (in a finite
computation time) accept in the case that $M$ halts on input 
$w$, or reject in the case that $M$ loops on input $w$. We can then
define a new Turing machine $Q$ that takes a single Turing machine $M$ 
as input. $Q$ will then ask $P$ whether machine $M$ halts when given
itself as input (does $P(\langle M, M \rangle)$ halt?). If $P$ 
accepts (says that $M$ halts) the $Q$ will loop. If $P$ rejects 
(says $M$ will loop) then $Q$ halts. Now, we can supply $Q$ as input
to machine $Q$. $Q$ will then run $P(\langle Q, Q \rangle)$. If $P$
accepts, then $Q$ will begin to loop, but $P$ said that $Q$ would
halt. This is a contraction, a general $P$ decider for the halting problem
cannot exist.

This same proof style as above can be used to perform a proof for any decider
from first principals. Just assume that the decider $P$ exists, generate a
machine $Q$ that takes a machine as input, the runs that decider on
the machine given itself as input $P(\langle M, M\rangle)$. Make machine $Q$
return the opposite of whatever $P$ says, and then pass $Q$ to $Q$. $P$ will
never be able to decide it.

\subsection{Mapping Reduction}
A mapping reduction between $A \subseteq \Sigma^*$ and 
$B \subseteq \Sigma^*$ is a function $f : \Sigma^* \to \Sigma^*$ if
$\forall x \in \Sigma^*, x \in A \Leftrightarrow f(x) \in B$. More plainly,
a function $f$ such that I can pick any $x$ in $A$, and $f(x)$ will also
be in $B$. The ``mapping reduction from $A$ to $B$'' is typically denoted
as $A \leq_m B$. A mapping reduction in polynomial time is denoted
$A \leq_p B$.

The general steps for a mapping reduction $A \leq_m B$ are as follows:
\begin{enumerate}
    \item $A$ is designated the ``known undecidable'' language.
    \item $B$ is designated the ``unknown'' language.
    \item Create a function $f$ that maps all elements of $A$ into $B$.
\end{enumerate}

To form $f$ you usually assume the decider for $B$ ($D_B$), then you construct
a machine $M$ that uses to decider $D_B$ to become a decider for $A$ ($D_A$).
For example, we can map $A_{TM}$ onto $Halt_{TM}$ using the following method:

Assume that a decider $R$ for $A_{TM}$ exists. We will now construct
a decider $S$ for $Halt_{TM}$ from $R$. $S$ has two inputs a machine $M$ and
an input string $w$. First, $S$ will run decider $R$ on $\langle M, w\rangle$,
if $R$ accepts, then $S$ accepts. If $R$ rejects, then $S$ accepts. We
now have a decider for $Halt_{TM}$ which is undecidable, a decider for
$A_{TM}$ cannot exist.

\subsection{Rice's Theorem}
``Every non-trivial partitioning of the space of Turing machine 
codes based on the languages recognized by these Turing machines 
is undecidable.''

More formally, given a property $\property$, where $\property$ is
non-trivial (not $\emptyset$ or $\Sigma^*$) the language below is
undecidable.
\[
    \{\langle M \rangle \spbar M \text{ is a Turing machine and } 
        \property(\langof(M))\}
\]

% PCP problem
\section{Post's Correspondence Problem}
Post's Correspondence Problem (PCP) is as follows. Say we have a finite set of
tuples in the form: $\{\langle x, y \rangle \spbar x, y \in \Sigma^*\}$. We can think
of these tuples like a set of blocks with a top half and a bottom half, where
the $x$ item is the top half and the $y$ item is the bottom half. Now to 
solve the PCP, find an arrangement (with possible repetitions) where the
string formed by the top blocks read left to right, is the same as the string
formed by the bottom blocks read left to right.

\subsection{Enumeration}
The set of solvable instances of this problem can be generated quite easily.
Make a machine that constantly generates two strings $w_x$ and $w_y$. A split
of $w_x$ (or $w_y$) is a set of strings that when re-ordered and concatenated forms
$w_x$ (or $w_y$). Now, make a set of all possible split of $w_x$ ($S_{w_x}$), and a set of
splits for $w_y$ ($S_{w_y}$). Then, cross every split in $S_{w_x}$  with every split
in $S_{w_y}$. Continue doing this for all possible strings 
$w_x \in \Sigma^*$, $w_y \in \Sigma^*$ where $|w_x| = |w_y|$.

\subsection{Decidability}
In the case of $|\Sigma| > 2$, the PCP is undecidable. This can be shown via
a mapping reduction that maps PCP into $A_{TM}$. The proof is rather involved, but
at a high-level, possible states of a Turing machine are expressed as PCP
pairs. A solution to the PCP problem containing these pairs is the \textit{execution
history} of the TM that accepted the string.

% NP-Completeness
\section{NP-Completeness}
\subsection{Problem Classes}
\begin{description}
    \item[{\small P:}] The set of problems that can be solved
    in polynomial time. Contained in NP.
    \item[{\small NP:}] The set of problems that can be solved
    in non-deterministic polynomial time.
    \item[{\small NP-hard:}] The set of problems that can be
    polynomial time reduced to every other problem in NP.
    \item[{\small NP-complete:}] The set of problems that are
    in both NP, and NP-hard.
\end{description}
\subsection{Proving NP-Completeness}
There are two steps to proving that a languages is in NP-complete.
First you have to show that it is NP, and then you have to show that
it is in NP-hard.

\subsubsection{Verifiers}
One way to show that a problem is in NP is by using a verifier. A verifier
is a Turing machine $V_L$ such that for all $w \in \Sigma^*$, their exists
some $c$ such that $w \in L$ when $V_L(w, c)$ accepts. Intuitively this can
be understood as ``There is a machine that can check the answers to problems
quickly''.

To give a concrete example, consider k-Clique. A verifier for k-Clique assumes
that $w$ is some graph, and $c$ is a set of nodes in graph $w$ that forms
a k-Clique. A machine that checks whether or not the set of nodes $c$ actually
forms a k-Clique is trivial and can easily be performed in a finite amount of
time.

\subsection{NP-hard Reduction}
The easiest way to show that a problem is in NP-hard, is by reducing an existing
NP-hard problem to the problem you want to prove is NP-hard using a polynomial  
mapping reduction. The classic problem to reduce from is 3-SAT.

\end{multicols*}
\end{document}
