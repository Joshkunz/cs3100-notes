% Context-Free languages:
% - CFL formalism
A CFL can be represented in two ways, with a Context Free Grammar (CFG) and with 
a Pushdown Automata (PDA). PDA's can be both deterministic and non-deterministic,
but unlike DFAs, Deterministic PDAs cannot represent the full set of CFLs. However,
any language that can be expressed as a DPDA is not inherently ambiguous.

The Formalism for PDAs is $(Q, \Sigma, \Gamma, \delta, q_0, z_0, F)$. 
It's basically the same as it is for DFAs with three changes. PDAs
have a stack alphabet $\Gamma$ which $\Sigma$ is often a subset of, but not
always. $z_0$ is the initial contents of the stack. The delta function is 
also changed to:
\[
    \delta : Q \times (\Sigma \cup \{\varepsilon\}) \times \Gamma 
    \to 2^{Q \times \Gamma^*}
\]
PDAs accept when either the stack becomes empty, or they reach a state in $F$.

\subsection{Closures}
Where $C$ is a context-free language, $R$ is a regular language, and $?$ is
an Unknown language.

\settowidth{\templength}{$C \cup C \spto C$}
\addtolength{\templength}{1cm}
\begin{tabular}{lp{\templength}l}
\textbf{Closed:} & & \textbf{Unclosed:} \\
$C^R \spto C$ & $C \cup C \spto C$ & $\overline{C} \spto ?$\\
$C^* \spto C$ & $C \cap R \spto C$ & $C \cap C \spto ?$\\
$CC \spto C$  & $C \cup R \spto R$ & $C \spbackslash C \spto ?$\\
$h(C) \spto C$ & & \\
\end{tabular}

% - Shortcuts for solving pumping lemmas
\subsection{Pumping Lemma}
\begin{align*}
  \exists N \in \mathbb{N}: & \\
          \forall w \in L : & |w| \geq N \impl \\
\exists uvxyz \in \Sigma^*: & \quad w = uvxyz \\
                            & \land |vy| > 0 \\
                            & \land |vxy| \leq N \\
                            & \land \forall i \geq 0: uv^ixy^iz \in L
\end{align*}

\subsection{CFG to PDA Conversion}
\begin{enumerate}
    \item Create an initial state $q_0$ with an initial value $z_0$ on the stack.
    \item Create a second state $q_M$. Add a transition when $z_0$ is on the
    top of the stack from $q_0$ to $q_M$. Push $z_0S$ onto the stack in this
    transition.
    \item Create a third state $q_F$, mark this state as final. Add a transition
    from $q_M$ to $q_F$ when $z_0$ is on the top of the stack.
    \item Now, populate $q_M$.
    \begin{itemize}
        \item Add a transition from $q_M$ to $q_M$ when there is a terminal 
        on top of the stack that matches the current input character.
        \item When there exists a rule
        $X \to R$ where $R$ can be a \textit{collection} of both terminals and
        non-terminals, add a transition from $q_M$ to $q_M$ when $X$ is on
        top of the stack that pushes $R$ onto the stack.
    \end{itemize}
\end{enumerate}

% - RL and LL to NFA/DFA conversion
%   - RL conversion via reversal closures
\subsection{RL and LL Grammar to NFA Conversion}
CFGs where every production only contains a single non-terminal on
the right (RL), or CFGs where every production only contains a single
non-terminal on the left (LL) can be directly converted to NFAs.
We'll with RL grammars. If a grammar is right linear, then we know
all the rules will be in the form $A \to tB$, or $A \to t$ where $t$ is
a terminal. The states of the NFA will be all the non-terminals.  A rule
like $A \to tB$ means that a transition from state $A$ to state $B$ on
input $t$ exists. A rule in the form $A \to t$ means that $A$ goes to a new
final state on input $t$.

To convert LL grammars to NFAs, we can convert them to RL grammars and then
to NFAs. A RL grammar can be obtained from a LL grammar by reversing it (reversing
the order of all productions. $Bt$ becomes $tB$). This
reversed RL can be converted to an NFA using the algorithm above. We can then
apply the NFA reversal algorithm to obtain an NFA for the original language.

\subsection{Cocke-Kasami-Younger (CKY) Parsing}
CKY is an algorithm for checking if a string is accepted by a given
CFG. For the CKY algorithm to be used, the CFG must be in Chomsky Normal form
(All productions in the form $S \to AB$, and $A \to t$).

The CKY algorithm helps you to build a binary parse-tree backwards. To start,
write out the characters of the string you're going to parse, and the non-terminals
that map to them above the characters. Then, in-between each non-terminal 
figure out what production would generate those non-terminals. For example if you
had a rule $S \to AB$, then you would write $S$ in-between non-terminal's $A$ and
$B$. This should build a triangle shape. If the point of the triangle is $S$, then
the string is accepted, if it's not $S$, then it didn't accept.

\subsection{Ambiguous Context Free Languages}
These are languages that have two separate parse-trees. To prove
that a language is ambiguous, show that it actually has two separate
parse-trees.

Example ambiguous grammar:
\[
    E \spto E + E \spbar E * E \spbar \texttt{NUMBER}
\]
% TODO: Needs graph example

\subsection{Consistency and Completeness}
\begin{description}
    \item[{\small Consistency:}] All strings generated by a grammar are 
    in the language.
    \item[{\small Completeness:}] The grammar generates all strings 
    in the language.
\end{description}

You cannot know that a grammar defines a language until you show both.
For example, if we want to define the language $\{a^nb^n | n \in \mathbb{N}\}$,
the grammar:
\[
    S \spto aabb
\]
Is \textit{consistent} because it only generates strings in the language,
but not complete because it doesn't generate all strings in the language.
Likewise, the grammar:
\[
    S \spto aS \spbar bS \spbar \varepsilon 
    \;\;\quad (\textit{grammar for }\{a, b\}^*)
\]
Is complete, it generates all possible strings in the language, but not
consistent because it generates many strings that are, in-fact, outside of
the language.
